{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3f21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file('./axial2.json')\n",
    "project = 'axial-module-393809'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1716b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.api_core.page_iterator.HTTPIterator object at 0x000001EDAFE89A20>\n",
      "<Blob: dlp-bucket-dev-axial, dlp_sample.xlsx, 1693994739910369>\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = 'dlp-bucket-dev-axial'\n",
    "storage_client = storage.Client(project = project, credentials = credentials)\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blobs = bucket.list_blobs()\n",
    "print(blobs)\n",
    "for b in blobs:\n",
    "    print(b)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2491e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845e1b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axial-module-393809 axial_dataset_sample sample_data\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project = \"axial-module-393809\", credentials=credentials)\n",
    "storage_client = storage.Client(project = project, credentials = credentials)\n",
    "\n",
    "dataset_id = \"axial-module-393809.axial_dataset_sample\"\n",
    "tables = client.list_tables(dataset_id)\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"{table.project} {table.dataset_id} {table.table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e62bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlp = google.cloud.dlp_v2.DlpServiceClient(credentials = credentials)\n",
    "\n",
    "project_id = \"axial-module-393809\"\n",
    "location_id = \"us-central1-c\"\n",
    "language_code = \"en-IN\"\n",
    "parent = f\"projects/{project_id}/languageCode/{language_code}\"\n",
    "response = dlp.list_info_types(\n",
    "                request={\"parent\": parent}\n",
    "            )\n",
    "# response.info_types.categories\n",
    "for infotype in response.info_types:\n",
    "    if infotype.categories:\n",
    "        for category in infotype.categories:\n",
    "#             print(str(category.location_category.name))\n",
    "            if category.location_category.name == \"INDIA\":\n",
    "                print(infotype.name)\n",
    "# print((response.info_types))\n",
    "# for info_type in response.info_types:\n",
    "#     print(len(info_type.name))\n",
    "#     print(f\"{info_type.name}: {info_type.display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74cdb6fb-4636-4d41-8e36-88dd5d9f4db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection operation started: projects/axial-module-393809/locations/global/dlpJobs/i-6341251296196257181\n",
      "No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to publish 1 messages.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 65, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\grpc\\_channel.py\", line 1161, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\grpc\\_channel.py\", line 1004, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INVALID_ARGUMENT\n",
      "\tdetails = \"One or more messages in the publish request is empty. Each message must contain either non-empty data, or at least one attribute.\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.195.42:443 {created_time:\"2023-09-14T05:29:45.9624137+00:00\", grpc_status:3, grpc_message:\"One or more messages in the publish request is empty. Each message must contain either non-empty data, or at least one attribute.\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\cloud\\pubsub_v1\\publisher\\_batch\\thread.py\", line 274, in _commit\n",
      "    response = self._client._gapic_publish(\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\cloud\\pubsub_v1\\publisher\\client.py\", line 267, in _gapic_publish\n",
      "    return super().publish(*args, **kwargs)\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\pubsub_v1\\services\\publisher\\client.py\", line 831, in publish\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 113, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\api_core\\retry.py\", line 349, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\api_core\\retry.py\", line 191, in retry_target\n",
      "    return target()\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\api_core\\timeout.py\", line 120, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\vbukka3\\OneDrive - DXC Production\\Documents\\dxc_github\\opsMate\\opsmate-env\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 67, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.InvalidArgument: 400 One or more messages in the publish request is empty. Each message must contain either non-empty data, or at least one attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: projects/axial-module-393809/locations/global/dlpJobs/i-6341251296196257181\n",
      "Info type: EMAIL_ADDRESS; Count: 3\n",
      "Info type: PASSWORD; Count: 3\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from typing import List, Optional\n",
    "import google.cloud.dlp\n",
    "import google.cloud.pubsub as pubsub\n",
    "\n",
    "project = \"axial-module-393809\"\n",
    "dlp = google.cloud.dlp_v2.DlpServiceClient(credentials=credentials)\n",
    "\n",
    "info_types = [\"FIRST_NAME\", \"LAST_NAME\", \"EMAIL_ADDRESS\", \"PASSWORD\"]\n",
    "info_types = [{\"name\": info_type} for info_type in info_types]\n",
    "\n",
    "inspect_config = {\n",
    "    \"info_types\": info_types,\n",
    "    \"min_likelihood\": \"LIKELY\",\n",
    "}\n",
    "\n",
    "storage_config = {\n",
    "    \"big_query_options\": {\n",
    "        \"table_reference\": {\n",
    "            \"project_id\": project,\n",
    "            \"dataset_id\": \"axial_dataset_sample\",\n",
    "            \"table_id\": \"sample_data\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "parent = f\"projects/{project}/locations/global\"\n",
    "topic_id = \"inspection-topic\"\n",
    "topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n",
    "actions = [{\"pub_sub\": {\"topic\": topic}}]\n",
    "\n",
    "inspect_job = {\n",
    "    \"inspect_config\": inspect_config,\n",
    "    \"storage_config\": storage_config,\n",
    "    \"actions\": actions,\n",
    "}\n",
    "\n",
    "operation = dlp.create_dlp_job(request={\"parent\": parent, \"inspect_job\": inspect_job})\n",
    "print(f\"Inspection operation started: {operation.name}\")\n",
    "\n",
    "subscription_id = \"inspection-sub\"\n",
    "subscriber = google.cloud.pubsub.SubscriberClient()\n",
    "subscription_path = subscriber.subscription_path(project, subscription_id)\n",
    "job_done = threading.Event()\n",
    "\n",
    "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n",
    "    try:\n",
    "        if message.attributes[\"DlpJobName\"] == operation.name:\n",
    "            message.ack()\n",
    "            message_body = message.data.decode(\"utf-8\")\n",
    "            publisher = pubsub.PublisherClient()\n",
    "            publisher.publish(topic, message_body.encode(\"utf-8\"))\n",
    "\n",
    "            job = dlp.get_dlp_job(request={\"name\": operation.name})\n",
    "            print(f\"Job name: {job.name}\")\n",
    "            if job.inspect_details.result.info_type_stats:\n",
    "                for finding in job.inspect_details.result.info_type_stats:\n",
    "                    print(\n",
    "                        \"Info type: {}; Count: {}\".format(\n",
    "                            finding.info_type.name, finding.count\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                print(\"No findings.\")\n",
    "\n",
    "            job_done.set()\n",
    "        else:\n",
    "            message.drop()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "timeout = 10\n",
    "subscriber.subscribe(subscription_path, callback=callback)\n",
    "finished = job_done.wait(timeout=timeout)\n",
    "if not finished:\n",
    "    print(\n",
    "        \"No event received before the timeout. Please verify that the \"\n",
    "        \"subscription provided is subscribed to the topic provided.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc63cd6-474c-438b-b1dd-f5a057609ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection operation started: projects/axial-module-393809/locations/global/dlpJobs/i-9101630489374998451\n",
      "DLP inspection completed.\n",
      "name: \"projects/axial-module-393809/locations/global/dlpJobs/i-9101630489374998451\"\n",
      "type_: INSPECT_JOB\n",
      "state: DONE\n",
      "inspect_details {\n",
      "  requested_options {\n",
      "    snapshot_inspect_template {\n",
      "    }\n",
      "    job_config {\n",
      "      storage_config {\n",
      "        big_query_options {\n",
      "          table_reference {\n",
      "            project_id: \"axial-module-393809\"\n",
      "            dataset_id: \"axial_dataset_sample\"\n",
      "            table_id: \"sample_data\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inspect_config {\n",
      "        info_types {\n",
      "          name: \"FIRST_NAME\"\n",
      "        }\n",
      "        info_types {\n",
      "          name: \"LAST_NAME\"\n",
      "        }\n",
      "        info_types {\n",
      "          name: \"EMAIL_ADDRESS\"\n",
      "        }\n",
      "        info_types {\n",
      "          name: \"PASSWORD\"\n",
      "        }\n",
      "        min_likelihood: LIKELY\n",
      "        limits {\n",
      "        }\n",
      "      }\n",
      "      actions {\n",
      "        save_findings {\n",
      "          output_config {\n",
      "            table {\n",
      "              project_id: \"axial-module-393809\"\n",
      "              dataset_id: \"result_dataset\"\n",
      "              table_id: \"result_table1\"\n",
      "            }\n",
      "            output_schema: BASIC_COLUMNS\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  result {\n",
      "    processed_bytes: 241\n",
      "    total_estimated_bytes: 241\n",
      "    info_type_stats {\n",
      "      info_type {\n",
      "        name: \"EMAIL_ADDRESS\"\n",
      "      }\n",
      "      count: 3\n",
      "    }\n",
      "    info_type_stats {\n",
      "      info_type {\n",
      "        name: \"PASSWORD\"\n",
      "      }\n",
      "      count: 3\n",
      "    }\n",
      "  }\n",
      "}\n",
      "create_time {\n",
      "  seconds: 1695205912\n",
      "  nanos: 377000000\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1695205930\n",
      "  nanos: 147112000\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1695205939\n",
      "  nanos: 82539000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import threading\n",
    "import google.cloud.dlp\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "from google.cloud import bigquery\n",
    "import time\n",
    "\n",
    "\n",
    "project = \"axial-module-393809\"\n",
    "findings_project_id = \"axial-module-393809\"\n",
    "findings_dataset_id = \"result_dataset\"\n",
    "findings_table_id = \"result_table1\"\n",
    "\n",
    "\n",
    "# Initialize DLP client\n",
    "dlp = google.cloud.dlp_v2.DlpServiceClient(credentials=credentials)\n",
    "bigquery_client = bigquery.Client(project=project)\n",
    "\n",
    "info_types = [\"FIRST_NAME\", \"LAST_NAME\", \"EMAIL_ADDRESS\", \"PASSWORD\"]\n",
    "info_types = [{\"name\": info_type} for info_type in info_types]\n",
    "\n",
    "inspect_config = {\n",
    "    \"info_types\": info_types,\n",
    "    \"min_likelihood\": \"LIKELY\",\n",
    "}\n",
    "\n",
    "storage_config = {\n",
    "    \"big_query_options\": {\n",
    "        \"table_reference\": {\n",
    "            \"project_id\": project,\n",
    "            \"dataset_id\": \"axial_dataset_sample\",\n",
    "            \"table_id\": \"sample_data\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "output_config = {\n",
    "    \"table\": {\n",
    "        \"project_id\": project,\n",
    "        \"dataset_id\": findings_dataset_id,\n",
    "        \"table_id\": findings_table_id,\n",
    "    },\n",
    "    \"output_schema\": \"BASIC_COLUMNS\",\n",
    "}\n",
    "\n",
    "save_findings_action = {\"save_findings\": {\"output_config\": output_config}}\n",
    "\n",
    "parent = f\"projects/{project}/locations/global\"\n",
    "\n",
    "inspect_job = {\n",
    "    \"inspect_config\": inspect_config,\n",
    "    \"storage_config\": storage_config,\n",
    "    \"actions\": [save_findings_action],\n",
    "}\n",
    "\n",
    "operation = dlp.create_dlp_job(request={\"parent\": parent, \"inspect_job\": inspect_job})\n",
    "print(f\"Inspection operation started: {operation.name}\")\n",
    "\n",
    "while True:\n",
    "    job = dlp.get_dlp_job(request={\"name\": operation.name})\n",
    "    if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n",
    "        break\n",
    "    time.sleep(5) \n",
    "\n",
    "print(\"DLP inspection completed.\")\n",
    "print(job)\n",
    "\n",
    "# if job.inspect_details and job.inspect_details.result:\n",
    "#     result = job.inspect_details.result\n",
    "\n",
    "#     processed_bytes = result.processed_bytes\n",
    "#     total_estimated_bytes = result.total_estimated_bytes\n",
    "#     info_type_stats = result.info_type_stats\n",
    "\n",
    "#     print(\"Processed Bytes:\", processed_bytes)\n",
    "#     print(\"Total Estimated Bytes:\", total_estimated_bytes)\n",
    "\n",
    "#     for info_type_stat in info_type_stats:\n",
    "#         info_type_name = info_type_stat.info_type.name\n",
    "#         count = info_type_stat.count\n",
    "#         print(\"Info Type:\", info_type_name)\n",
    "#         print(\"Count:\", count)\n",
    "# else:\n",
    "#     print(\"No DLP findings found.\")\n",
    "\n",
    "\n",
    "# schema = [\n",
    "#     bigquery.SchemaField(\"Processed_Bytes\", \"INTEGER\"),\n",
    "#     bigquery.SchemaField(\"Total_Estimated_Bytes\", \"INTEGER\"),\n",
    "#     bigquery.SchemaField(\"Info_Type\", \"STRING\"),\n",
    "#     bigquery.SchemaField(\"Count\", \"INTEGER\"),\n",
    "# ]\n",
    "\n",
    "# dataset_ref = bigquery_client.dataset(bq_dataset)\n",
    "# table_ref = dataset_ref.table(bq_table)\n",
    "\n",
    "# try:\n",
    "#     table = bigquery.Table(table_ref, schema=schema)\n",
    "#     table = bigquery_client.create_table(table)\n",
    "#     print(f\"Table {project}.{bq_dataset}.{bq_table} created.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Table {project}.{bq_dataset}.{bq_table} already exists.\")\n",
    "\n",
    "# rows_to_insert = []\n",
    "\n",
    "# if job.inspect_details and job.inspect_details.result:\n",
    "#     result = job.inspect_details.result\n",
    "\n",
    "#     processed_bytes = result.processed_bytes\n",
    "#     total_estimated_bytes = result.total_estimated_bytes\n",
    "#     info_type_stats = result.info_type_stats\n",
    "\n",
    "#     for info_type_stat in info_type_stats:\n",
    "#         info_type_name = info_type_stat.info_type.name\n",
    "#         count = info_type_stat.count\n",
    "#         rows_to_insert.append(\n",
    "#             (processed_bytes, total_estimated_bytes, info_type_name, count)\n",
    "#         )\n",
    "\n",
    "# if rows_to_insert:\n",
    "#     try:\n",
    "#         errors = bigquery_client.insert_rows(table, rows_to_insert)\n",
    "#         if not errors:\n",
    "#             print(f\"DLP findings have been stored in {project}.{bq_dataset}.{bq_table}.\")\n",
    "#         else:\n",
    "#             print(f\"Error inserting rows into BigQuery: {errors}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error inserting rows into BigQuery: {str(e)}\")\n",
    "# else:\n",
    "#     print(\"No DLP findings to store in BigQuery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccef1d3-009d-4782-b5b9-8289b4c8aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
